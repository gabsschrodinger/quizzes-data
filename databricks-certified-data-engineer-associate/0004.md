## Metadata:

- id: 0004
- topic: Databricks Workflows
- subtopic: General
- amountCorrect: 1
- amountIncorrect: 3

## Question:

You are tasked with creating a data pipeline in Databricks Workflows. The pipeline should include the following steps:

1. Read data from a Delta Lake table.
2. Transform the data using a Python notebook.
3. Write the transformed data back to a different Delta Lake table.

What is the correct sequence for setting up this pipeline?

## Answers:

### Correct answers:

#### Answer 1:

- Set up a Databricks Workflow job.
- Use separate tasks for reading, transforming, and writing data.

### Incorrect answers:

#### Answer 1:

- Use a Databricks SQL query to read and write data.
- Include a Python notebook for transformations.

#### Answer 2:

- Use a Databricks Delta Live Table (DLT) pipeline to define all steps.

#### Answer 3:

- Create a Spark job that includes all steps.

## Explanation:

Databricks Workflows allows you to define pipelines with multiple tasks (e.g., notebooks, SQL queries) and manage dependencies between them.
